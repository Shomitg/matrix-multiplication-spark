{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries and initiate the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# initiate the spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('yarn') \\\n",
    "    .appName('matrix-multiplication') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to retain the original row indices after multiplying twoÂ matrices, we'll add a few dummy values to the original matrices so that we can reproduce the original row indices as the first column of the resulting matrix after multiplication.\n",
    "\n",
    "<img src=\"img/1_matrix_multiplication.png\" alt=\"matrix multiplication\" style=\"width: 1000px; height: 175px;\"/>\n",
    "<img src=\"img/2_matrix_multiplication_with_dummy_values.png\" alt=\"matrix multiplication with dummy values\" style=\"width: 1000px; height: 175px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the guest feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of guest feature matrix: 1000000 x 101\n"
     ]
    }
   ],
   "source": [
    "# create the guest feature matrix (an IndexedRowMatrix)\n",
    "# first column in the matrix is composed of the dummy values\n",
    "# corresponding to the actual row indices\n",
    "num_guests = 1000000\n",
    "guest_feature_vector_length = 100\n",
    "\n",
    "guest_feature_rows = spark.sparkContext.parallelize(range(num_guests), 100) \\\n",
    "    .map(lambda guest_id: IndexedRow(\n",
    "        guest_id, [guest_id] + list(np.random.randint(low = 1, high = 10, size = guest_feature_vector_length))\n",
    "    ))\n",
    "\n",
    "guest_feature_matrix = IndexedRowMatrix(guest_feature_rows.repartition(200))\n",
    "print(f'dimensions of guest feature matrix: {guest_feature_matrix.numRows()} x {guest_feature_matrix.numCols()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create the item feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of item feature matrix: 101 x 1001\n"
     ]
    }
   ],
   "source": [
    "# create the item feature matrix (a local LocalMatrix)\n",
    "# first column and first row in the matrix is composed of the dummy values\n",
    "num_items = 1000\n",
    "item_feature_vector_length = 100\n",
    "\n",
    "# first row\n",
    "item_feature_array = [1] + [0] * item_feature_vector_length\n",
    "\n",
    "# rest of the rows\n",
    "for i in range(num_items):\n",
    "    item_feature_array += [0] + list(np.random.randint(low = 1, high = 10, size = item_feature_vector_length))\n",
    "item_feature_matrix = Matrices.dense(item_feature_vector_length + 1, num_items + 1, item_feature_array)\n",
    "print(f'dimensions of item feature matrix: {item_feature_vector_length + 1} x {num_items + 1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Multiply guest features and item features to get the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of ratings matrix: 1000000 x 1001\n"
     ]
    }
   ],
   "source": [
    "# calculate the guest-item rating matrix by multiplying\n",
    "# the guest feature matrix with item feature matrix\n",
    "ratings_matrix = guest_feature_matrix.multiply(item_feature_matrix)\n",
    "print(f'dimensions of ratings matrix: {ratings_matrix.numRows()} x {ratings_matrix.numCols()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+--------------------+\n",
      "|raw_guest_index|dummy_guest_index|  guest_rating_array|\n",
      "+---------------+-----------------+--------------------+\n",
      "|         418290|           178850|[2170.0, 2487.0, ...|\n",
      "|         418294|           178854|[2049.0, 2349.0, ...|\n",
      "|         461013|           671413|[2141.0, 2292.0, ...|\n",
      "|         461014|           671414|[2364.0, 2602.0, ...|\n",
      "|         461019|           671419|[2415.0, 2653.0, ...|\n",
      "|         742063|           563233|[2192.0, 2628.0, ...|\n",
      "|         742068|           563238|[2177.0, 2673.0, ...|\n",
      "|         619911|           128971|[2563.0, 2807.0, ...|\n",
      "|         898620|           229150|[2238.0, 2456.0, ...|\n",
      "|         709719|           968309|[2389.0, 2627.0, ...|\n",
      "+---------------+-----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the guest rating vectors\n",
    "ratings_rdd = ratings_matrix.rows \\\n",
    "    .repartition(500) \\\n",
    "    .map(lambda ele: (ele.index, ele.vector.toArray().tolist())) \\\n",
    "    .map(lambda ele: (int(ele[0]), int(ele[1][0]), ele[1][1:]))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('raw_guest_index', IntegerType(), True),\n",
    "    StructField('dummy_guest_index', IntegerType(), True),\n",
    "    StructField('guest_rating_array', ArrayType(DoubleType()), True)\n",
    "])\n",
    "\n",
    "ratings = spark.createDataFrame(ratings_rdd, schema)\n",
    "ratings.filter(col('raw_guest_index') != col('dummy_guest_index')).sample(0.1).show(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
